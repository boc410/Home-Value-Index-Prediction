{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ebc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f487d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df = pd.read_pickle('../Data/all_metro_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9f2301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metro_id</th>\n",
       "      <th>metro_name</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_name</th>\n",
       "      <th>period</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>metro_for_sale_inventory</th>\n",
       "      <th>metro_hvi</th>\n",
       "      <th>metro_med_days_to_close</th>\n",
       "      <th>...</th>\n",
       "      <th>metro_pct_blw_list</th>\n",
       "      <th>metro_pct_w_pricecut</th>\n",
       "      <th>metro_rent</th>\n",
       "      <th>state_hvi</th>\n",
       "      <th>state_personal_income</th>\n",
       "      <th>state_personal_income_per_capita</th>\n",
       "      <th>state_population</th>\n",
       "      <th>state_gdp_cur_dol</th>\n",
       "      <th>state_job_openings</th>\n",
       "      <th>state_job_openings_szn_adjd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>394297</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>SD</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2009-02-28</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128414.895204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147638.885390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394297</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>SD</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128428.614787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147559.476539</td>\n",
       "      <td>31406.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394297</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>SD</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2009-04-30</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128232.347262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147229.826738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394297</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>SD</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2009-05-31</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128120.247626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146863.651913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>394297</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>SD</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2009-06-30</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127924.063926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146483.638609</td>\n",
       "      <td>31711.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212452</th>\n",
       "      <td>845172</td>\n",
       "      <td>Winfield, KS</td>\n",
       "      <td>KS</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222566.236372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212453</th>\n",
       "      <td>845172</td>\n",
       "      <td>Winfield, KS</td>\n",
       "      <td>KS</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>146.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.215387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222324.630758</td>\n",
       "      <td>190108.6</td>\n",
       "      <td>64591.0</td>\n",
       "      <td>2943265.0</td>\n",
       "      <td>64591.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212454</th>\n",
       "      <td>845172</td>\n",
       "      <td>Winfield, KS</td>\n",
       "      <td>KS</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>136.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.194851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222197.238452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212455</th>\n",
       "      <td>845172</td>\n",
       "      <td>Winfield, KS</td>\n",
       "      <td>KS</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222731.307862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212456</th>\n",
       "      <td>845172</td>\n",
       "      <td>Winfield, KS</td>\n",
       "      <td>KS</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.193936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224240.995918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212457 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metro_id    metro_name state_id    state_name     period  year  month  \\\n",
       "0        394297  Aberdeen, SD       SD  South Dakota 2009-02-28  2009      2   \n",
       "1        394297  Aberdeen, SD       SD  South Dakota 2009-03-31  2009      3   \n",
       "2        394297  Aberdeen, SD       SD  South Dakota 2009-04-30  2009      4   \n",
       "3        394297  Aberdeen, SD       SD  South Dakota 2009-05-31  2009      5   \n",
       "4        394297  Aberdeen, SD       SD  South Dakota 2009-06-30  2009      6   \n",
       "...         ...           ...      ...           ...        ...   ...    ...   \n",
       "212452   845172  Winfield, KS       KS        Kansas 2023-11-30  2023     11   \n",
       "212453   845172  Winfield, KS       KS        Kansas 2023-12-31  2023     12   \n",
       "212454   845172  Winfield, KS       KS        Kansas 2024-01-31  2024      1   \n",
       "212455   845172  Winfield, KS       KS        Kansas 2024-02-29  2024      2   \n",
       "212456   845172  Winfield, KS       KS        Kansas 2024-03-31  2024      3   \n",
       "\n",
       "        metro_for_sale_inventory      metro_hvi  metro_med_days_to_close  ...  \\\n",
       "0                            NaN  128414.895204                      NaN  ...   \n",
       "1                            NaN  128428.614787                      NaN  ...   \n",
       "2                            NaN  128232.347262                      NaN  ...   \n",
       "3                            NaN  128120.247626                      NaN  ...   \n",
       "4                            NaN  127924.063926                      NaN  ...   \n",
       "...                          ...            ...                      ...  ...   \n",
       "212452                     154.0            NaN                      NaN  ...   \n",
       "212453                     146.0            NaN                      NaN  ...   \n",
       "212454                     136.0            NaN                      NaN  ...   \n",
       "212455                     130.0            NaN                      NaN  ...   \n",
       "212456                     121.0            NaN                      NaN  ...   \n",
       "\n",
       "        metro_pct_blw_list  metro_pct_w_pricecut  metro_rent      state_hvi  \\\n",
       "0                      NaN                   NaN         NaN  147638.885390   \n",
       "1                      NaN                   NaN         NaN  147559.476539   \n",
       "2                      NaN                   NaN         NaN  147229.826738   \n",
       "3                      NaN                   NaN         NaN  146863.651913   \n",
       "4                      NaN                   NaN         NaN  146483.638609   \n",
       "...                    ...                   ...         ...            ...   \n",
       "212452                 NaN              0.267385         NaN  222566.236372   \n",
       "212453                 NaN              0.215387         NaN  222324.630758   \n",
       "212454                 NaN              0.194851         NaN  222197.238452   \n",
       "212455                 NaN              0.186402         NaN  222731.307862   \n",
       "212456                 NaN              0.193936         NaN  224240.995918   \n",
       "\n",
       "        state_personal_income  state_personal_income_per_capita  \\\n",
       "0                         NaN                               NaN   \n",
       "1                     31406.1                               NaN   \n",
       "2                         NaN                               NaN   \n",
       "3                         NaN                               NaN   \n",
       "4                     31711.7                               NaN   \n",
       "...                       ...                               ...   \n",
       "212452                    NaN                               NaN   \n",
       "212453               190108.6                           64591.0   \n",
       "212454                    NaN                               NaN   \n",
       "212455                    NaN                               NaN   \n",
       "212456                    NaN                               NaN   \n",
       "\n",
       "        state_population  state_gdp_cur_dol  state_job_openings  \\\n",
       "0                    NaN                NaN                 8.0   \n",
       "1                    NaN                NaN                 7.0   \n",
       "2                    NaN                NaN                 8.0   \n",
       "3                    NaN                NaN                10.0   \n",
       "4                    NaN                NaN                 8.0   \n",
       "...                  ...                ...                 ...   \n",
       "212452               NaN                NaN                83.0   \n",
       "212453         2943265.0            64591.0                77.0   \n",
       "212454               NaN                NaN                 NaN   \n",
       "212455               NaN                NaN                 NaN   \n",
       "212456               NaN                NaN                 NaN   \n",
       "\n",
       "        state_job_openings_szn_adjd  \n",
       "0                               9.0  \n",
       "1                               7.0  \n",
       "2                               8.0  \n",
       "3                               9.0  \n",
       "4                               8.0  \n",
       "...                             ...  \n",
       "212452                         87.0  \n",
       "212453                         85.0  \n",
       "212454                          NaN  \n",
       "212455                          NaN  \n",
       "212456                          NaN  \n",
       "\n",
       "[212457 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metro_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efedf4",
   "metadata": {},
   "source": [
    "# Create lag features, normalize numeric data and split into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33acaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lag features for trends\n",
    "y_cols = ['metro_id','metro_hvi','state_hvi']\n",
    "X_cols = ['metro_id', 'metro_for_sale_inventory', 'metro_hvi', 'metro_med_days_to_close', \n",
    "          'metro_new_construct', 'metro_new_listing', 'metro_pct_abv_list', 'metro_pct_blw_list', \n",
    "          'metro_pct_w_pricecut', 'metro_rent', 'state_hvi', 'state_personal_income',\n",
    "          'state_personal_income_per_capita', 'state_population', 'state_gdp_cur_dol', 'state_job_openings', 'state_job_openings_szn_adjd']\n",
    "\n",
    "def add_lag_features(df, fwrd_lag=[1,3,6,12], prev_lag=[1,3,6,12]):\n",
    "    global y_new_cols\n",
    "    global X_new_cols\n",
    "    y_new_cols = []\n",
    "    X_new_cols = []\n",
    "    \n",
    "    for lag in fwrd_lag:\n",
    "\n",
    "        #Add hvi values for future periods to be used as target variable\n",
    "        df = df.sort_values(by=['metro_id','state_id','period'])\n",
    "        y_shifted_cols = [\"frwd\"+str(lag)+\"_mon_\"+col for col in y_cols]\n",
    "        y_new_cols.extend(y_shifted_cols)\n",
    "\n",
    "        df[y_shifted_cols] = df[y_cols].shift(-lag) #create forward looking columns\n",
    "        df.loc[metro_df[y_cols[0]] != df[y_shifted_cols[0]],y_shifted_cols[1:3]] = np.nan #set new numeric columns as null if metro_id doesn't match\n",
    "        df = df.drop(columns=[y_shifted_cols[0]]) #drop metro_id duplicate column\n",
    "\n",
    "        #Create column with percentage change from the lag period\n",
    "        for i, col in enumerate(y_shifted_cols):\n",
    "            if 'metro_id' in col:\n",
    "                pass\n",
    "            else:\n",
    "                old_col = y_cols[i]\n",
    "                new_col_nm = col+\"_pct_chg\"\n",
    "                df[new_col_nm] = (df[col] / df[old_col]) - 1\n",
    "                y_new_cols.extend([new_col_nm])\n",
    "\n",
    "    #Add incremental changes from previous periods\n",
    "    for lag in prev_lag:\n",
    "        X_shifted_cols = [\"prev\"+str(lag)+\"_mon_\"+col for col in X_cols]\n",
    "        X_new_cols.extend(X_shifted_cols)\n",
    "\n",
    "        df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
    "        df.loc[metro_df[X_cols[0]] != df[X_shifted_cols[0]],X_shifted_cols[1:3]] = np.nan #set new numeric columns as null if metro_id doesn't match\n",
    "        df = df.drop(columns=[X_shifted_cols[0]]) #drop metro_id duplicate column\n",
    "\n",
    "        #Create column with percentage change from the lag period\n",
    "        for i, col in enumerate(X_shifted_cols):\n",
    "            if 'metro_id' in col:\n",
    "                pass\n",
    "            else:\n",
    "                old_col = X_cols[i]\n",
    "                new_col_nm = col+\"_pct_chg\"\n",
    "                df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
    "                X_new_cols.extend([new_col_nm])\n",
    "    \n",
    "    y_new_cols = [col for col in y_new_cols if 'metro_id' not in col] \n",
    "    X_new_cols = [col for col in X_new_cols if 'metro_id' not in col]\n",
    "    return_cols = ['metro_id', 'period'] + X_new_cols + y_new_cols\n",
    "    return(df[return_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ecb2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def train_normalizer(df, scaler=scaler):\n",
    "    #Seperate columns into types for reshaping of data\n",
    "    orig_columns = list(df.columns)\n",
    "    metro_columns = orig_columns[:4] + ['variable']\n",
    "    non_numeric_features = orig_columns[:7]\n",
    "    \n",
    "    #reshape data so periods are each a seperate columns\n",
    "    df_reshaped = pd.melt(df, id_vars=non_numeric_features)\n",
    "    df_reshaped = df_reshaped.pivot(index=metro_columns,\n",
    "                                    columns='period',\n",
    "                                    values='value')\n",
    "\n",
    "    #Forward fill data\n",
    "    #For periods with missing data, fill with the previous value available. If no early data available, leave null.\n",
    "    df_reshaped = df_reshaped.ffill(axis=1)\n",
    "    \n",
    "    #Normalize all numeric data to retain variance but range from 0 to 1\n",
    "    scaler.fit(df_reshaped.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef4419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, scaler=scaler):\n",
    "    #Seperate columns into types for reshaping of data\n",
    "    orig_columns = list(df.columns)\n",
    "    merge_columns = orig_columns[:4] + ['period']\n",
    "    metro_columns = orig_columns[:4] + ['variable']\n",
    "    period_columns = orig_columns[4:7]\n",
    "    numeric_features = orig_columns[7:]\n",
    "    non_numeric_features = orig_columns[:7]\n",
    "    \n",
    "    #reshape data so periods are each a seperate columns\n",
    "    df_reshaped = pd.melt(df, \n",
    "                          id_vars=non_numeric_features)\n",
    "    df_reshaped = df_reshaped.pivot(index=metro_columns,\n",
    "                                    columns='period',\n",
    "                                    values='value')\n",
    "\n",
    "    #Forward fill data\n",
    "    #For periods with missing data, fill with the previous value available. If no early data available, leave null.\n",
    "    df_reshaped = df_reshaped.ffill(axis=1)\n",
    "    \n",
    "    #Reshape filled data back to original format\n",
    "    df_unshaped = pd.melt(df_reshaped.reset_index(), \n",
    "                          id_vars=metro_columns)\n",
    "    df_unshaped = df_unshaped.pivot(index=merge_columns,\n",
    "                                    columns='variable',\n",
    "                                    values='value')\n",
    "\n",
    "    \n",
    "    #Normalize all numeric data to retain variance but range from 0 to 1\n",
    "    data_normalized = scaler.transform(df_reshaped.T)\n",
    "    df_normalized = pd.DataFrame(data_normalized.T, \n",
    "                                 index=df_reshaped.index, \n",
    "                                 columns=df_reshaped.columns\n",
    "                                ).reset_index()\n",
    "\n",
    "    #Reshape back to original format with normalized data\n",
    "    df_normalized = pd.melt(df_normalized,\n",
    "                            id_vars=metro_columns)\n",
    "    df_normalized = df_normalized.pivot(index=merge_columns,\n",
    "                                        columns='variable',\n",
    "                                        values='value'\n",
    "                                       ).add_suffix('_normalized')\n",
    "    \n",
    "    #Add normalized data to original dataframe as seperate columns\n",
    "    df_final = df_unshaped.merge(df_normalized, \n",
    "                                 on=merge_columns, \n",
    "                                 how='right'\n",
    "                                ).reset_index()\n",
    "    \n",
    "    #Clean up year and month columns\n",
    "    df_final.insert(5,'month',df_final['period'].dt.month)\n",
    "    df_final.insert(5,'year',df_final['period'].dt.year)\n",
    "    return(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc06b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[X_shifted_cols] = df[X_cols].shift(lag) #create backward looking columns\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n",
      "C:\\Users\\Brendan\\AppData\\Local\\Temp\\ipykernel_14044\\2496223725.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_nm] = (df[old_col] / df[col]) - 1\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe with lag features\n",
    "metro_df_lag_features = add_lag_features(metro_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b62fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets for time series split cross-validation\n",
    "\n",
    "#Sample 1 (Train: 2000-2013.06 | Test: 2013.07-2014.12) 1.5 of 23 years (6%)\n",
    "#Sample 2 (Train: 2000-2014 | Test: 2015.01-2016.06) 1.5 of 23 years (6%)\n",
    "#Sample 3 (Train: 2000-2016.06 | Test: 2016.07-2018.12) 1.5 of 23 years (6%)\n",
    "#Validation (Train: 2000-2018 | Test: 2019-2023) 4 of 23 years (17%)\n",
    "\n",
    "sample_groups = ['metro_samp_1','metro_samp_2','metro_samp_3','metro_samp_val']\n",
    "sample_train = [sample + '_train' for sample in sample_groups]\n",
    "sample_test = [sample + '_test' for sample in sample_groups]\n",
    "train_start_date = ['2000-01-01','2000-01-01','2000-01-01','2000-01-01']\n",
    "train_end_date = ['2013-07-01','2015-01-01','2016-07-01','2019-01-01']\n",
    "test_start_date = ['2013-07-01','2015-01-01','2016-07-01','2019-01-01']\n",
    "test_end_date = ['2015-01-01','2016-07-01','2019-01-01','2024-01-01']\n",
    "\n",
    "sample_dfs = {}\n",
    "for i, df_name in enumerate(sample_groups):\n",
    "    df = metro_df[(metro_df.period >= train_start_date[i]) &\n",
    "                  (metro_df.period <= test_end_date[i])]\n",
    "\n",
    "    sample_dfs[df_name] = df\n",
    "    \n",
    "    df_train_name = df_name + '_train'\n",
    "    df_train = df[(df.period >= train_start_date[i]) &\n",
    "                  (df.period <= train_end_date[i])]\n",
    "    sample_dfs[df_train_name] = df_train\n",
    "\n",
    "    df_test_name = df_name + '_test'\n",
    "    df_test = df[(df.period >= test_start_date[i]) &\n",
    "                 (df.period <= test_end_date[i]) &\n",
    "                 (df.metro_id.isin(df_train.metro_id))] #exclude any metro id's that are not in the training dataset\n",
    "    sample_dfs[df_test_name] = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a0ecd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:473: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:474: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:473: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:474: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:473: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:474: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:473: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:474: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "#Normalize all datasets and add lag features\n",
    "for i, df_name in enumerate(sample_groups):\n",
    "    #train normalizer\n",
    "    df_train_name = df_name + '_train'\n",
    "    df_train_name_normalized = df_train_name + '_normalized'\n",
    "    train_normalizer(sample_dfs[df_train_name])\n",
    "    \n",
    "    #normalize training data\n",
    "    df_train_normalized = normalize_data(sample_dfs[df_train_name])\n",
    "    df_train_normalized = df_train_normalized.merge(metro_df_lag_features, on=['metro_id','period'], how='left')\n",
    "    sample_dfs[df_train_name_normalized] = df_train_normalized\n",
    "    \n",
    "    #normalize test data\n",
    "    df_test_name = df_name + '_test'\n",
    "    df_test_name_normalized = df_test_name + '_normalized'\n",
    "    df_test_normalized = normalize_data(sample_dfs[df_test_name])\n",
    "    df_test_normalized = df_test_normalized.merge(metro_df_lag_features, on=['metro_id','period'], how='left')\n",
    "    sample_dfs[df_test_name_normalized] = df_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452d1e2",
   "metadata": {},
   "source": [
    "# Cluster metro areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9260f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Brendan\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Set features for clustering\n",
    "kmeans_features = ['metro_id',\n",
    "                   'metro_name',\n",
    "                   'period',\n",
    "                   'metro_hvi',\n",
    "                   'prev12_mon_metro_hvi_pct_chg',\n",
    "                   'metro_new_construct',\n",
    "                   'prev12_mon_metro_rent_pct_chg',\n",
    "                   'state_hvi', \n",
    "                   'state_job_openings', \n",
    "                   'state_personal_income',\n",
    "                   'state_population']\n",
    "\n",
    "#Drop records with missing data\n",
    "kmeans_df = sample_dfs['metro_samp_val_train_normalized'][kmeans_features].dropna().copy()\n",
    "kmeans_data_clean = np.array(kmeans_df[kmeans_features[3:]])\n",
    "\n",
    "#Standardize data to have consistent mean and variance for each feature\n",
    "standardizer = StandardScaler()\n",
    "standardizer.fit(kmeans_data_clean)\n",
    "kmeans_data_standardized = standardizer.transform(kmeans_data_clean)\n",
    "\n",
    "#use kmeans model to find 5 buckets for metro id's using key features identified during EDA \n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "metro_clusters = kmeans.fit_predict(kmeans_data_standardized)\n",
    "\n",
    "#create dictionary to lookup cluster for each metro_id\n",
    "kmeans_df['cluster'] = metro_clusters\n",
    "metro_clusters_df = kmeans_df.groupby(['metro_id','metro_name']).cluster.value_counts(normalize=True).reset_index().drop_duplicates(subset=['metro_id'])\n",
    "metro_clusters_dict = metro_clusters_df.set_index('metro_id')['cluster'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6cc8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add dummy columns for clusters from kmeans\n",
    "for sample_df in sample_dfs:\n",
    "    #Remove old clusters (if rerunning)\n",
    "    cols = sample_dfs[sample_df].columns\n",
    "    drop_cols = [col for col in cols if 'cluster' in col]\n",
    "    \n",
    "    #Add clusters and generate dummy columns\n",
    "    sample_dfs[sample_df] = sample_dfs[sample_df].drop(columns=drop_cols)\n",
    "    sample_dfs[sample_df]['cluster'] = sample_dfs[sample_df]['metro_id'].map(metro_clusters_dict).fillna(5).astype('int') #add 6th cluster for missing metros\n",
    "    sample_dfs[sample_df] = pd.get_dummies(sample_dfs[sample_df],columns=['cluster'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be931098",
   "metadata": {},
   "source": [
    "# Export all train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbe69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle the dictionary of DataFrames to a file\n",
    "with open('../Data/test_train_datasets.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_dfs, f)  # 'wb' for writing in binary mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
